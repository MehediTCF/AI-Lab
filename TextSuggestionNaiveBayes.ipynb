{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSuggestionNaiveBayes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp7xAzPSAxAM"
      },
      "source": [
        "#importing dependencies\r\n",
        "import nltk\r\n",
        "from nltk import bigrams,trigrams \r\n",
        "from nltk.corpus import reuters\r\n",
        "from gensim.test.utils import datapath\r\n",
        "from gensim.corpora import WikiCorpus\r\n",
        "from collections import Counter, defaultdict\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gm6OLKaBLOq",
        "outputId": "45299edb-03d1-4181-96b7-67c332c9afda"
      },
      "source": [
        "WikiDatasetPath = datapath('enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2')\r\n",
        "Sentences = WikiCorpus(WikiDatasetPath).get_texts()\r\n",
        "print(Sentences)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object WikiCorpus.get_texts at 0x7f84320b6a40>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFskQmWSCh2o",
        "outputId": "3832bc51-290f-4611-92e7-7f1dd2b86139"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "nltk.download('reuters')\r\n",
        "reutersSentences  = reuters.sents()\r\n",
        "print(reutersSentences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-pmCI0OClrY"
      },
      "source": [
        "def calculateWordProbability(sentenceModel):\r\n",
        "  for nextWord in sentenceModel:\r\n",
        "    nextWords = sentenceModel[nextWord]\r\n",
        "    total_Word_Count = float(sum(nextWords.values()))\r\n",
        "    for previousWord in nextWords:\r\n",
        "      nextWords[previousWord]/=total_Word_Count"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCOhAZEFCzs4"
      },
      "source": [
        "def calculateSigleWordProbability(sentenceModel,wordCount):\r\n",
        "  for word in sentenceModel:\r\n",
        "    sentenceModel[word]/=wordCount"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWRHANrlC3V4"
      },
      "source": [
        "#convert to lowercase\r\n",
        "def convertToLower(pa):\r\n",
        "  if type(pa)==str:\r\n",
        "    return pa.lower()\r\n",
        "  return pa"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRNg6xj4C_p2"
      },
      "source": [
        "sentenceModel4 = defaultdict(lambda: set())\r\n",
        "sentenceModel5 = defaultdict(lambda: set())\r\n",
        "\r\n",
        "def calculateWordCount(sentenceModel1,sentenceModel2,sentenceModel3,sentences):\r\n",
        "  wordCount = 0\r\n",
        "  for sentence in sentences:\r\n",
        "    #print(sentence)\r\n",
        "    for word in sentence:\r\n",
        "      wordCount+=1\r\n",
        "      sentenceModel1[word]+=1\r\n",
        "    for previousWord2,previousWord1,nextWord in trigrams(sentence,pad_right=True,pad_left=True):\r\n",
        "      previousWord1 = convertToLower(previousWord1)\r\n",
        "      previousWord2 = convertToLower(previousWord2)\r\n",
        "      # print(previousWord1)\r\n",
        "      # print(previousWord2)\r\n",
        "      nextWord = convertToLower(nextWord)\r\n",
        "      sentenceModel2[nextWord][previousWord1]+=1\r\n",
        "      sentenceModel3[nextWord][previousWord2]+=1\r\n",
        "      sentenceModel4[previousWord1].add(nextWord)\r\n",
        "      sentenceModel5[previousWord2].add(nextWord)\r\n",
        "\r\n",
        "  return wordCount"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OLlxSsVDEip"
      },
      "source": [
        "sentenceModel1 = defaultdict(lambda:0)\r\n",
        "sentenceModel2 = defaultdict(lambda: defaultdict(lambda:0))\r\n",
        "sentenceModel3 = defaultdict(lambda: defaultdict(lambda:0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q5zbbIeDIk6",
        "outputId": "cc84a872-83e1-4016-ce40-553d4399fc21"
      },
      "source": [
        "wikiWordCount = calculateWordCount(sentenceModel1,sentenceModel2,sentenceModel3,Sentences)\r\n",
        "print(wikiWordCount)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "452944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_35iaJwDOX0",
        "outputId": "addf7dbf-f9e9-4c7b-8e95-3337706b841c"
      },
      "source": [
        "reutersWordCount = calculateWordCount(sentenceModel1,sentenceModel2,sentenceModel3,reutersSentences)\r\n",
        "print(reutersWordCount)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1720917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQEb3WWDV0x"
      },
      "source": [
        "calculateWordProbability(sentenceModel2)\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwYLSjoZDohj"
      },
      "source": [
        "calculateWordProbability(sentenceModel3)\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "884YRLHvDs9T"
      },
      "source": [
        "total_word = wikiWordCount + reutersWordCount\r\n",
        "calculateSigleWordProbability(sentenceModel1,total_word)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGyJpTrjD1dQ"
      },
      "source": [
        "ProbabilityWords = []\r\n",
        "def makeWordSuggestionByTrigram(previousWord2,previousWord1):\r\n",
        "  for nextWord in sentenceModel4[previousWord1] & sentenceModel5[previousWord2]:\r\n",
        "    naiveBiasTrigramWeight = sentenceModel1[nextWord]*sentenceModel2[nextWord][previousWord1]*sentenceModel3[nextWord][previousWord2]\r\n",
        "    ProbabilityWords.append((nextWord,naiveBiasTrigramWeight))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIE1cQmID7ku",
        "outputId": "090f9d6a-028c-40d2-eb65-9ebe422a9b7f"
      },
      "source": [
        "makeWordSuggestionByTrigram('my','name')\r\n",
        "ProbabilityWords.sort(key=lambda o:o[1],reverse=True)\r\n",
        "print(*ProbabilityWords[:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('is', 2.684929364088014e-08) ('to', 1.1753701280369164e-09) (',', 5.59438489695973e-10) ('in', 4.6278217257881707e-10) ('and', 4.5063509855288127e-10) ('for', 3.738790783749001e-10) ('would', 3.5383441884561533e-10) ('will', 2.8735045390861837e-10) ('that', 2.0690529880634479e-10) ('or', 1.711572663102755e-10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aCHsZy-EDDC",
        "outputId": "c48f4abb-6fb6-4f99-c3de-3a0b0eafd279"
      },
      "source": [
        "while(True):\r\n",
        "    text = input(\"Enter sentence: \")\r\n",
        "    if text == \"stop\":\r\n",
        "        print(\"Program ending...\")\r\n",
        "        break\r\n",
        "    \r\n",
        "    else:\r\n",
        "        try:\r\n",
        "            ProbabilityWords = []\r\n",
        "            text = text.split(\" \")\r\n",
        "            makeWordSuggestionByTrigram(text[0],text[1])\r\n",
        "            ProbabilityWords.sort(key=lambda o:o[1],reverse=True)\r\n",
        "            print(*ProbabilityWords[:10])\r\n",
        "            \r\n",
        "        except:\r\n",
        "            continue"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter sentence: hi \n",
            "\n",
            "Enter sentence: i am\n",
            "('afraid', 1.0350247784931974e-06) ('sure', 8.036310732474202e-07) ('astonished', 4.600110126636432e-07) ('speculating', 3.6800881013091455e-07) ('deeply', 1.8818632336239947e-07) ('convinced', 1.5333700422121436e-07) ('confident', 1.2893042223444673e-07) ('hopeful', 4.600110126636432e-08) ('inclined', 4.600110126636432e-08) ('sceptical', 3.833425105530359e-08)\n",
            "Enter sentence: stop\n",
            "Program ending...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}